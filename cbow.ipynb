{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Classification\" data-toc-modified-id=\"Text-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Subjectivity-Dataset\" data-toc-modified-id=\"Subjectivity-Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Subjectivity Dataset</a></span></li><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Tokenization\" data-toc-modified-id=\"Simple-Tokenization-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Simple Tokenization</a></span></li><li><span><a href=\"#Much-better-tokenization-with-Spacy\" data-toc-modified-id=\"Much-better-tokenization-with-Spacy-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Much better tokenization with Spacy</a></span></li></ul></li><li><span><a href=\"#Split-dataset-in-train-and-test\" data-toc-modified-id=\"Split-dataset-in-train-and-test-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Split dataset in train and test</a></span></li><li><span><a href=\"#Word-to-index-mapping\" data-toc-modified-id=\"Word-to-index-mapping-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Word to index mapping</a></span></li><li><span><a href=\"#Sentence-encoding\" data-toc-modified-id=\"Sentence-encoding-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Sentence encoding</a></span></li><li><span><a href=\"#Embedding-layer\" data-toc-modified-id=\"Embedding-layer-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Embedding layer</a></span></li><li><span><a href=\"#Continuous-Bag-of-Words-Model\" data-toc-modified-id=\"Continuous-Bag-of-Words-Model-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Continuous Bag of Words Model</a></span></li></ul></li><li><span><a href=\"#Training-the-CBOW-model\" data-toc-modified-id=\"Training-the-CBOW-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training the CBOW model</a></span></li><li><span><a href=\"#Data-loaders-for-SGD\" data-toc-modified-id=\"Data-loaders-for-SGD-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data loaders for SGD</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "In this part of the tutorial we develop a continuous bag of words (CBOW) model for a text classification task described [here]( https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf). The CBOW model was first described [here](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Dataset\n",
    "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:\n",
    "```\n",
    "wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-20 15:16:44--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519599 (507K) [application/x-gzip]\n",
      "Saving to: ‘rotten_imdb.tar.gz’\n",
      "\n",
      "rotten_imdb.tar.gz  100%[===================>] 507.42K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2019-02-20 15:16:45 (47.5 MB/s) - ‘rotten_imdb.tar.gz’ saved [519599/519599]\n",
      "\n",
      "x quote.tok.gt9.5000\n",
      "x plot.tok.gt9.5000\n",
      "x subjdata.README.1.0\n"
     ]
    }
   ],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n"
     ]
    }
   ],
   "source": [
    "! head -2 data/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/plot.tok.gt9.5000'),\n",
       " PosixPath('data/subjdata.README.1.0'),\n",
       " PosixPath('data/quote.tok.gt9.5000')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the task of chopping up text into pieces, called tokens.\n",
    "\n",
    "spaCy is an open-source software library for advanced Natural Language Processing. Here we will use it for tokenization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need each line in the file \n",
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a',\n",
       "       'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi',\n",
       "       'from', 'a', 'hunter', '.'], dtype='<U8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(obj_lines[0].strip().lower().split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time run this\n",
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tok(obj_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([x for x in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
    "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smart and alert , thirteen conversations about one thing is a small gem .',\n",
       " 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['will god let her fall or give her a new path ?',\n",
       "        \"the director's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship ( most notably wretched sound design ) .\",\n",
       "        \"welles groupie/scholar peter bogdanovich took a long time to do it , but he's finally provided his own broadside at publishing giant william randolph hearst .\",\n",
       "        'based on the 1997 john king novel of the same name with a rather odd synopsis : \" a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules .',\n",
       "        'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack .'],\n",
       "       dtype='<U691'), array([1., 0., 0., 1., 1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index mapping\n",
    "In interest of time we will tokenize without spaCy. Here we will compute a vocabulary of words based on the training set and a mapping from word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the vocabulary from the training set\n",
    "word_count = get_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21415"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's delete words that are very infrequent\n",
    "for word in list(word_count):\n",
    "    if word_count[word] < 5:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4065"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we need an index for each word in the vocab\n",
    "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
    "words = [\"<PAD>\", \"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence encoding\n",
    "Here we encode each sentence as a sequence of indices corresponding to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_len = np.array([len(x.split()) for x in X_train])\n",
    "x_test_len = np.array([len(x.split()) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(x_train_len, 95) # let set the max sequence len to N=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will god let her fall or give her a new path ?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the index of the word or the index of \"UNK\" otherwise\n",
    "vocab2index.get(\"?\", vocab2index[\"UNK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12, 11,  5,  3,  4,  9,  5, 10,  6,  2,  7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=40):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12, 11,  5,  3,  4,  9,  5, 10,  6,  2,  7,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 40)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 40)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([encode_sentence(x) for x in X_test])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer\n",
    "Most deep learning models use a dense vectors of real numbers as representation of words (word embeddings), as opposed to a one-hot encoding representations. The module torch.nn.Embedding is used to represent word embeddings. It takes two arguments: the vocabulary size, and the dimensionality of the embeddings. The embeddings are initialized with random vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "        [ 0.1901,  1.3434,  2.4007, -1.3473],\n",
       "        [ 1.9391,  0.4154,  1.8454, -0.0440],\n",
       "        [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "        [-1.2015,  0.6970, -0.4598,  0.1874],\n",
       "        [-0.5489,  1.1735, -1.6129, -0.3163],\n",
       "        [ 1.2211, -0.1239,  0.3409, -1.1177],\n",
       "        [ 0.5130, -1.4622,  0.4938,  0.4620],\n",
       "        [-0.0666,  1.7076, -2.7429, -0.1095]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 words with embedding size 4\n",
    "# embedding will be initialized at random\n",
    "embed = nn.Embedding(10, 4, padding_idx=0)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `padding_idx` has embedding vector 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-1.2015,  0.6970, -0.4598,  0.1874],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "# can you see that some vectors are the same?\n",
    "a = torch.LongTensor([[1,4,1,5,1,0]])\n",
    "embed(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the representation of a sentence with words with indices [1,4,1,5,1] and a padding at the end. Bellow we have an example in which we have two sentences. the first sentence has length 3 and the last sentence has length 2. In order to use a tensor we use padding at the end of the second sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[1,4,1], [1,3,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model takes an average of the word embedding of each word. Here is how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.FloatTensor([3, 2]) # here is the size of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877]],\n",
       "\n",
       "        [[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [ 1.9391,  0.4154,  1.8454, -0.0440],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8529,  2.2012, -0.9765,  0.4752],\n",
       "        [ 2.7927,  0.5506,  1.5726,  0.4437]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(a).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2843,  0.7337, -0.3255,  0.1584],\n",
       "        [ 1.3964,  0.2753,  0.7863,  0.2219]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embs = embed(a).sum(dim=1) \n",
    "sum_embs/ s.view(s.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_size, 1)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.sum(dim=1)/ s.view(s.shape[0], 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size=5, emb_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.6823, -1.3827, -0.1621],\n",
       "        [-0.0323, -0.2556,  2.5525],\n",
       "        [-1.2646, -0.0038,  1.4429],\n",
       "        [-0.5084, -0.4949, -0.3695]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5973],\n",
       "        [ 0.7232]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CBOW model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067\n"
     ]
    }
   ],
   "source": [
    "V = len(words)\n",
    "model = CBOW(vocab_size=V, emb_size=50)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model):\n",
    "    model.eval()\n",
    "    x = torch.LongTensor(x_test) #.cuda()\n",
    "    y = torch.Tensor(y_test).unsqueeze(1) #).cuda()\n",
    "    s = torch.Tensor(x_test_len).view(x_test_len.shape[0], 1)\n",
    "    y_hat = model(x, s)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.item(), accuracy.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.702 and accuracy 0.477\n"
     ]
    }
   ],
   "source": [
    "# accuracy of a random model should be around 0.5\n",
    "test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        x = torch.LongTensor(x_train)  #.cuda()\n",
    "        y = torch.Tensor(y_train).unsqueeze(1)\n",
    "        s = torch.Tensor(x_train_len).view(x_train_len.shape[0], 1)\n",
    "        y_hat = model(x, s)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7024547457695007\n",
      "0.6619945168495178\n",
      "0.5833789110183716\n",
      "0.4843262732028961\n",
      "0.37453681230545044\n",
      "0.2894887924194336\n",
      "0.2270020842552185\n",
      "0.1904212087392807\n",
      "0.16081099212169647\n",
      "0.14118488132953644\n",
      "test loss 0.264 and accuracy 0.902\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12197427451610565\n",
      "0.11815891414880753\n",
      "0.11501367390155792\n",
      "0.1116267591714859\n",
      "0.10820414870977402\n",
      "0.10499171167612076\n",
      "0.10200171917676926\n",
      "0.09909720718860626\n",
      "0.09619797766208649\n",
      "0.09334372729063034\n",
      "test loss 0.263 and accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create a data loader. The data loader provides the following features:\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence2(s, N=40):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12,  6,  9,  2, 10,  4,  3,  2,  8,  5, 11,  7,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0], dtype=int32), 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence2(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectivityDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        x, s = encode_sentence2(x)\n",
    "        return x, self.y[idx], s\n",
    "    \n",
    "sub_dataset_train = SubjectivityDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=5, shuffle=True)\n",
    "x, y, s = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  118,    16,  1273,  1359,    60,  3632,    60,    97,    28,\n",
       "            931,    64,     8,  1258,    30,    37,   497,  1035,     1,\n",
       "             18,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    1,     1,     1,    17,     1,   475,   869,    60,     1,\n",
       "              1,    30,    60,    59,    82,    16,     1,    51,     1,\n",
       "              1,    28,   137,  2143,  2285,    18,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    8,   128,   758,  1742,     1,  1834,     1,  3657,    17,\n",
       "              1,    18,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  824,    43,     8,   730,     1,    17,     8,     1,    51,\n",
       "              1,     1,     1,     1,    27,     1,    30,  1539,    30,\n",
       "              1,   215,    28,    90,     1,    30,    67,   105,   305,\n",
       "             18,    17,    95,  1716,     8,  2382,    79,    16,    89,\n",
       "             51,    16,     1,  1498],\n",
       "         [    1,   152,    16,  1250,  2118,    72,  3636,    30,    16,\n",
       "              1,    59,     1,    43,  4010,    18,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]], dtype=torch.int32),\n",
       " tensor([ 1.,  1.,  0.,  0.,  1.], dtype=torch.float64),\n",
       " tensor([ 19,  24,  11,  40,  15]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size=V, emb_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for x, y, s in train_loader:\n",
    "            x = x.type(torch.LongTensor)  #.cuda()\n",
    "            y = y.type(torch.FloatTensor).unsqueeze(1)\n",
    "            s = s.type(torch.Tensor).view(s.shape[0], 1)\n",
    "            y_hat = model(x, s)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss.item())\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595077395439148\n",
      "0.4045970141887665\n",
      "0.26646387577056885\n",
      "0.19933322072029114\n",
      "0.16005471348762512\n",
      "0.140223428606987\n",
      "0.12022583186626434\n",
      "0.11802416294813156\n",
      "0.08472650498151779\n",
      "0.08180411905050278\n",
      "0.08018757402896881\n",
      "0.08355165272951126\n",
      "0.06486327946186066\n",
      "0.05094480887055397\n",
      "0.051483701914548874\n",
      "0.04123803228139877\n",
      "0.03242563083767891\n",
      "0.02988939918577671\n",
      "0.028660159558057785\n",
      "0.025195850059390068\n",
      "test loss 0.336 and accuracy 0.890\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "116px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
