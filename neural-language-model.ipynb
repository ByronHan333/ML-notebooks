{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with a language model\n",
    "This model is a version of the one introduced in 2003 by Bengio et all [here](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded here.\n",
    "`https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.test.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/mode117.pth')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n"
     ]
    }
   ],
   "source": [
    "!head -4 /data/yinterian/wikitext-2/wiki.train.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = read_file(PATH/'wiki.train.tokens')\n",
    "valid_lines = read_file(PATH/'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {}\n",
    "words = []\n",
    "for word in vocab:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an \"unk\" already\n",
    "vocab2index['<unk>'] #[y for y in x for x in non_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in train_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in valid_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2051961,), (213886,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content.shape, val_content.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Example say we have this dataset and we are using a window size of window=3.\n",
    "\n",
    "Raw Dataset:\n",
    "`the cat is walking in the bedroom`\n",
    "\n",
    "`x                  y \n",
    "the cat is         walking \n",
    "cat is walking     in \n",
    "is walking in      the \n",
    "walking in the     bedroom` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, content, window=5):\n",
    "        self.content = content\n",
    "        self.window = window\n",
    "        self.len = len(self.content) - self.window\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.content[idx : idx + self.window]\n",
    "        y = self.content[idx + self.window]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset([0,1,2,3,4,5,6], window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "([0, 1, 2], 3)\n",
      "([1, 2, 3], 4)\n",
      "([2, 3, 4], 5)\n",
      "([3, 4, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for i in range(4): \n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WikiDataset(train_content, window=5)\n",
    "val_ds = WikiDataset(val_content, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # testing model\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    \"\"\"Lang Model that needs to be regularized.\"\"\"\n",
    "\n",
    "    def __init__(self, V, D, hidden, window=5, dropout=0.5):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.word_emb = nn.Embedding(V, D)\n",
    "        self.linear1 = nn.Linear(window*D, hidden)\n",
    "        self.dense_bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, V)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.dense_bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  143, 12674,     5,     3,     3],\n",
       "         [   59,  3071,     7,   188,    65],\n",
       "         [  337,    65,  4681,  2103, 10499],\n",
       "         [   83, 11647,    65,  1748, 11866]]), tensor([  3, 247,  72,   5]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 3 # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = nn.Embedding(V, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1896,  0.3813, -1.0437],\n",
       "         [-2.5420,  1.1579,  1.5254],\n",
       "         [-0.4364,  0.0629, -1.2389],\n",
       "         [-0.6150, -0.6007, -1.0478],\n",
       "         [-0.6150, -0.6007, -1.0478]],\n",
       "\n",
       "        [[ 0.4863,  0.2596,  0.5871],\n",
       "         [-1.0439, -0.5869, -1.2581],\n",
       "         [-0.5645, -0.5120,  0.8133],\n",
       "         [-0.8718,  0.2595,  0.7135],\n",
       "         [-0.5934,  1.4313, -0.8278]],\n",
       "\n",
       "        [[ 0.4297,  0.7937,  0.9868],\n",
       "         [-0.5934,  1.4313, -0.8278],\n",
       "         [-1.0084, -0.4667,  1.2358],\n",
       "         [ 0.8987,  0.0916, -0.9026],\n",
       "         [-0.3846,  1.4131,  1.8568]],\n",
       "\n",
       "        [[ 0.4838, -0.4012, -0.3694],\n",
       "         [ 0.7983,  0.4171,  0.2032],\n",
       "         [-0.5934,  1.4313, -0.8278],\n",
       "         [ 0.7160, -1.1515,  1.8237],\n",
       "         [-0.6277, -0.1911, -1.5470]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = word_emb(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1896,  0.3813, -1.0437, -2.5420,  1.1579,  1.5254, -0.4364,  0.0629,\n",
       "         -1.2389, -0.6150, -0.6007, -1.0478, -0.6150, -0.6007, -1.0478],\n",
       "        [ 0.4863,  0.2596,  0.5871, -1.0439, -0.5869, -1.2581, -0.5645, -0.5120,\n",
       "          0.8133, -0.8718,  0.2595,  0.7135, -0.5934,  1.4313, -0.8278],\n",
       "        [ 0.4297,  0.7937,  0.9868, -0.5934,  1.4313, -0.8278, -1.0084, -0.4667,\n",
       "          1.2358,  0.8987,  0.0916, -0.9026, -0.3846,  1.4131,  1.8568],\n",
       "        [ 0.4838, -0.4012, -0.3694,  0.7983,  0.4171,  0.2032, -0.5934,  1.4313,\n",
       "         -0.8278,  0.7160, -1.1515,  1.8237, -0.6277, -0.1911, -1.5470]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(x1.shape[0], -1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2638,  0.0213,  0.1190,  ..., -0.2202, -0.3349,  0.0259],\n",
       "        [ 0.2928, -0.3325,  0.4515,  ..., -0.3163, -0.6589, -0.1121],\n",
       "        [ 0.1962,  0.2472,  0.0485,  ..., -0.2298, -0.5431, -0.1626],\n",
       "        [-0.0775,  0.0145, -0.5498,  ...,  0.4065,  0.3443,  0.6282]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`F.cross_entropy` combines `log_softmax` and `nll_loss` in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        total_loss = 0.\n",
    "        for x, y in train_dl:\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss +=  x.size(0)* loss.item()\n",
    "            total += x.size(0)\n",
    "        val_loss = val_metrics(model)\n",
    "        print(\"train_loss %.3f val_loss %.3f perplexity %.3f\" % (total_loss/total, \n",
    "                                                                 val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    total_loss = 0.\n",
    "    for x,y in val_dl:\n",
    "        x = torch.LongTensor(x).cuda()\n",
    "        y = torch.LongTensor(y).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        total_loss +=  x.size(0)* loss.item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.440684290446596"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 6.583 val_loss 5.904 perplexity 366.390\n",
      "train_loss 5.819 val_loss 5.610 perplexity 273.271\n",
      "train_loss 5.609 val_loss 5.448 perplexity 232.385\n",
      "train_loss 5.480 val_loss 5.341 perplexity 208.666\n",
      "train_loss 5.389 val_loss 5.261 perplexity 192.649\n",
      "train_loss 5.319 val_loss 5.199 perplexity 181.113\n",
      "train_loss 5.263 val_loss 5.144 perplexity 171.485\n",
      "train_loss 5.215 val_loss 5.100 perplexity 164.055\n",
      "train_loss 5.174 val_loss 5.062 perplexity 157.843\n",
      "train_loss 5.138 val_loss 5.026 perplexity 152.298\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 5.173 val_loss 5.014 perplexity 150.565\n",
      "train_loss 5.081 val_loss 4.969 perplexity 143.948\n",
      "train_loss 5.048 val_loss 4.940 perplexity 139.776\n",
      "train_loss 5.020 val_loss 4.914 perplexity 136.227\n",
      "train_loss 4.996 val_loss 4.890 perplexity 132.946\n",
      "train_loss 4.973 val_loss 4.869 perplexity 130.176\n",
      "train_loss 4.953 val_loss 4.849 perplexity 127.601\n",
      "train_loss 4.934 val_loss 4.830 perplexity 125.264\n",
      "train_loss 4.916 val_loss 4.813 perplexity 123.074\n",
      "train_loss 4.899 val_loss 4.795 perplexity 120.900\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model20.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_dl, lr_low=1e-3, lr_high=1, epochs=5):\n",
    "    losses = []\n",
    "    iterations = epochs * len(train_dl)\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    losses = []\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    for i in range(epochs):\n",
    "        for x, y in train_dl:\n",
    "            lr = lrs[ind]\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            losses.append(loss.item())\n",
    "            ind += 1\n",
    "    return lrs, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()\n",
    "lrs, losses = LR_range_finder(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(lrs, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangular_lr2(lr_low, lr_high, iterations):\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "    \n",
    "def train_triangular_policy(model, epochs=4, lr_low=1e-4, lr_high=4):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    losses = []\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, iterations)\n",
    "    \n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    for i in range(epochs):\n",
    "        for x, y in train_dl:\n",
    "            lr = lrs[ind]\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            losses.append(loss.item())\n",
    "            ind += 1\n",
    "    \n",
    "    idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[idx]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += len(data)*loss.data\n",
    "            idx += 1\n",
    "        # results after each epoch\n",
    "        val_loss = evaluate(val_data)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_loss = total_loss[0]/len(train_data)\n",
    "        print('| epoch {:3d} | lr {:02.5f} | t_loss {:5.2f} | t_ppl {:5.2f} | v_loss {:5.2f} | v_ppl {:5.2f}'.format(\n",
    "             epoch, lr, train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=2, lr_high=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.5, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.001, lr_high=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p):\n",
    "    torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "\n",
    "p = PATH/\"mode117.pth\"\n",
    "save_model(model, str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode117.pth\"\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
