{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with a language model\n",
    "This model is a version of the one introduced in 2003 by Bengio et all [here](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded here.\n",
    "`https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/wikitext-2/model118.pth'),\n",
       " PosixPath('/data2/yinterian/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/data2/yinterian/wikitext-2/mode.pth'),\n",
       " PosixPath('/data2/yinterian/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/data2/yinterian/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data2/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n"
     ]
    }
   ],
   "source": [
    "!head -4 /data2/yinterian/wikitext-2/wiki.train.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = read_file(PATH/'wiki.train.tokens')\n",
    "valid_lines = read_file(PATH/'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {}\n",
    "words = []\n",
    "for word in vocab:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an \"unk\" already\n",
    "vocab2index['<unk>'] #[y for y in x for x in non_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in train_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in valid_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2051961,), (213886,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content.shape, val_content.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Example say we have this dataset and we are using a window size of window=3.\n",
    "\n",
    "Raw Dataset:\n",
    "`the cat is walking in the bedroom`\n",
    "\n",
    "`x                  y \n",
    "the cat is         walking \n",
    "cat is walking     in \n",
    "is walking in      the \n",
    "walking in the     bedroom` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, content, window=5):\n",
    "        self.content = content\n",
    "        self.window = window\n",
    "        self.len = len(self.content) - self.window\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.content[idx : idx + self.window]\n",
    "        y = self.content[idx + self.window]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset([0,1,2,3,4,5,6], window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "([0, 1, 2], 3)\n",
      "([1, 2, 3], 4)\n",
      "([2, 3, 4], 5)\n",
      "([3, 4, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for i in range(4): \n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WikiDataset(train_content, window=5)\n",
    "val_ds = WikiDataset(val_content, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "batch_size = 4 # testing model\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    \"\"\"Lang Model that needs to be regularized.\"\"\"\n",
    "\n",
    "    def __init__(self, V, D, hidden, window=5, dropout=0.5):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.word_emb = nn.Embedding(V, D)\n",
    "        self.linear1 = nn.Linear(window*D, hidden)\n",
    "        self.dense_bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, V)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.dense_bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   73,  9824, 16692,  2346,    52],\n",
       "         [  662,  1033,  3839,    55,   515],\n",
       "         [16792,  1437,    64,   647,    10],\n",
       "         [   73,  1437,  1030,  7653,    46]]),\n",
       " tensor([  346,    55,   314, 21261]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 3 # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = nn.Embedding(V, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0499, -0.3318,  1.1019],\n",
       "         [ 0.9620, -0.6670, -0.6020],\n",
       "         [ 0.0566,  0.1635,  1.5894],\n",
       "         [-0.5709,  0.7283, -1.6596],\n",
       "         [ 0.1506, -1.0360, -1.4777]],\n",
       "\n",
       "        [[ 0.8292,  0.3812,  0.0967],\n",
       "         [ 1.5746,  1.1332,  0.8757],\n",
       "         [ 0.0695,  0.9323,  1.2294],\n",
       "         [ 0.2022, -0.0868,  0.0973],\n",
       "         [ 0.2537, -0.4287,  0.2558]],\n",
       "\n",
       "        [[ 0.0434,  0.3347,  0.6777],\n",
       "         [ 1.5693,  0.1856,  2.3612],\n",
       "         [ 2.6062,  0.8953,  2.3783],\n",
       "         [ 0.5960,  0.4311,  1.3122],\n",
       "         [-0.0049,  2.3764, -0.4857]],\n",
       "\n",
       "        [[-0.0499, -0.3318,  1.1019],\n",
       "         [ 1.5693,  0.1856,  2.3612],\n",
       "         [-0.3882,  0.5207,  1.1808],\n",
       "         [-0.7995,  1.2070, -0.1370],\n",
       "         [ 0.5958, -0.6289,  0.9748]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = word_emb(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0499, -0.3318,  1.1019,  0.9620, -0.6670, -0.6020,  0.0566,  0.1635,\n",
       "          1.5894, -0.5709,  0.7283, -1.6596,  0.1506, -1.0360, -1.4777],\n",
       "        [ 0.8292,  0.3812,  0.0967,  1.5746,  1.1332,  0.8757,  0.0695,  0.9323,\n",
       "          1.2294,  0.2022, -0.0868,  0.0973,  0.2537, -0.4287,  0.2558],\n",
       "        [ 0.0434,  0.3347,  0.6777,  1.5693,  0.1856,  2.3612,  2.6062,  0.8953,\n",
       "          2.3783,  0.5960,  0.4311,  1.3122, -0.0049,  2.3764, -0.4857],\n",
       "        [-0.0499, -0.3318,  1.1019,  1.5693,  0.1856,  2.3612, -0.3882,  0.5207,\n",
       "          1.1808, -0.7995,  1.2070, -0.1370,  0.5958, -0.6289,  0.9748]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(x1.shape[0], -1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1251, -0.1488, -0.2374,  ...,  0.3231, -0.0897, -0.2084],\n",
       "        [ 0.2631,  0.0883,  0.5152,  ...,  0.2035, -0.1961, -0.1148],\n",
       "        [ 0.0987, -0.2718,  0.0898,  ...,  0.1490,  0.0106, -0.0042],\n",
       "        [ 0.3349, -0.0773,  0.1611,  ..., -0.0981,  0.0718, -0.4433]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`F.cross_entropy` combines `log_softmax` and `nll_loss` in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "batch_size = 4 # testing model\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for x,y in train_dl:\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss.item())\n",
    "    #val_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_data, lr_low=1e-3, lr_high=10, epochs=2):\n",
    "    losses = []\n",
    "    (train_data.size(0) - 1)//bptt + 1\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    losses = []\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for i in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[ind]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "        \n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            losses.append(loss.data[0])\n",
    "            ind += 1\n",
    "    return lrs, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "lrs, losses = LR_range_finder(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5968"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(lrs, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangular_lr2(lr_low, lr_high, iterations):\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    return Variable(h.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "\n",
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "    \n",
    "def train_triangular_policy(model, epochs=4, lr_low=1e-4, lr_high=4):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, iterations)\n",
    "    idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[idx]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += len(data)*loss.data\n",
    "            idx += 1\n",
    "        # results after each epoch\n",
    "        val_loss = evaluate(val_data)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_loss = total_loss[0]/len(train_data)\n",
    "        print('| epoch {:3d} | lr {:02.5f} | t_loss {:5.2f} | t_ppl {:5.2f} | v_loss {:5.2f} | v_ppl {:5.2f}'.format(\n",
    "             epoch, lr, train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 8.28489 | t_loss  6.52 | t_ppl 676.54 | v_loss  5.88 | v_ppl 356.71\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.81 | t_ppl 334.17 | v_loss  5.46 | v_ppl 234.26\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.47 | t_ppl 237.65 | v_loss  5.29 | v_ppl 198.54\n",
      "| epoch   3 | lr 4.00000 | t_loss  5.30 | t_ppl 200.86 | v_loss  5.21 | v_ppl 182.89\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 8.28489 | t_loss  5.23 | t_ppl 186.70 | v_loss  5.25 | v_ppl 191.28\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.21 | t_ppl 183.43 | v_loss  5.15 | v_ppl 173.28\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.05 | t_ppl 155.99 | v_loss  5.05 | v_ppl 156.07\n",
      "| epoch   3 | lr 4.00000 | t_loss  4.95 | t_ppl 141.00 | v_loss  5.01 | v_ppl 150.53\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=4, lr_high=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 6.28489 | t_loss  4.89 | t_ppl 133.02 | v_loss  5.03 | v_ppl 152.95\n",
      "| epoch   1 | lr 5.42878 | t_loss  4.92 | t_ppl 137.17 | v_loss  5.01 | v_ppl 150.15\n",
      "| epoch   2 | lr 2.63954 | t_loss  4.81 | t_ppl 122.90 | v_loss  4.95 | v_ppl 140.68\n",
      "| epoch   3 | lr 2.00000 | t_loss  4.75 | t_ppl 115.03 | v_loss  4.92 | v_ppl 136.56\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=2, lr_high=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.72 | t_ppl 111.86 | v_loss  4.96 | v_ppl 142.24\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.75 | t_ppl 116.10 | v_loss  4.93 | v_ppl 138.41\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.68 | t_ppl 107.77 | v_loss  4.89 | v_ppl 132.51\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.64 | t_ppl 103.22 | v_loss  4.88 | v_ppl 131.12\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.63 | t_ppl 102.69 | v_loss  4.94 | v_ppl 139.68\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.68 | t_ppl 107.69 | v_loss  4.91 | v_ppl 135.64\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.61 | t_ppl 100.48 | v_loss  4.88 | v_ppl 131.01\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.57 | t_ppl 96.27 | v_loss  4.86 | v_ppl 128.77\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 4.57074 | t_loss  4.57 | t_ppl 96.16 | v_loss  4.94 | v_ppl 139.16\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.62 | t_ppl 101.75 | v_loss  4.90 | v_ppl 134.75\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.55 | t_ppl 94.81 | v_loss  4.87 | v_ppl 130.20\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.51 | t_ppl 90.76 | v_loss  4.84 | v_ppl 126.99\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=1, lr_high=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.99952 | t_loss  4.50 | t_ppl 89.72 | v_loss  4.87 | v_ppl 130.73\n",
      "| epoch   1 | lr 2.50012 | t_loss  4.53 | t_ppl 92.41 | v_loss  4.87 | v_ppl 130.96\n",
      "| epoch   2 | lr 0.87306 | t_loss  4.48 | t_ppl 88.63 | v_loss  4.85 | v_ppl 127.19\n",
      "| epoch   3 | lr 0.50000 | t_loss  4.46 | t_ppl 86.83 | v_loss  4.83 | v_ppl 124.72\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.5, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.85945 | t_loss  4.46 | t_ppl 86.33 | v_loss  4.88 | v_ppl 131.15\n",
      "| epoch   1 | lr 2.29014 | t_loss  4.49 | t_ppl 89.12 | v_loss  4.86 | v_ppl 129.46\n",
      "| epoch   2 | lr 0.43529 | t_loss  4.46 | t_ppl 86.08 | v_loss  4.81 | v_ppl 122.88\n",
      "| epoch   3 | lr 0.01000 | t_loss  4.47 | t_ppl 86.95 | v_loss  4.77 | v_ppl 118.41\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 2.14531 | t_loss  4.43 | t_ppl 83.85 | v_loss  4.86 | v_ppl 129.56\n",
      "| epoch   1 | lr 1.71867 | t_loss  4.45 | t_ppl 85.22 | v_loss  4.86 | v_ppl 128.80\n",
      "| epoch   2 | lr 0.32870 | t_loss  4.43 | t_ppl 83.67 | v_loss  4.81 | v_ppl 122.69\n",
      "| epoch   3 | lr 0.01000 | t_loss  4.44 | t_ppl 85.17 | v_loss  4.77 | v_ppl 117.78\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.01, lr_high=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 1.42858 | t_loss  4.41 | t_ppl 82.25 | v_loss  4.84 | v_ppl 126.83\n",
      "| epoch   1 | lr 1.14335 | t_loss  4.41 | t_ppl 82.16 | v_loss  4.83 | v_ppl 125.69\n",
      "| epoch   2 | lr 0.21407 | t_loss  4.41 | t_ppl 81.95 | v_loss  4.79 | v_ppl 120.86\n",
      "| epoch   3 | lr 0.00100 | t_loss  4.44 | t_ppl 84.46 | v_loss  4.77 | v_ppl 117.45\n"
     ]
    }
   ],
   "source": [
    "train_triangular_policy(model, epochs=4, lr_low=0.001, lr_high=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p):\n",
    "    torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "\n",
    "p = PATH/\"mode117.pth\"\n",
    "save_model(model, str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yinterian/wikitext-2/mode117.pth\n"
     ]
    }
   ],
   "source": [
    "p = PATH/\"mode117.pth\"\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
