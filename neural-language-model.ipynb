{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with a language model\n",
    "This model is a version of the one introduced in 2003 by Bengio et all [here](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded here.\n",
    "`https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.test.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model20.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model10.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/mode117.pth')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n"
     ]
    }
   ],
   "source": [
    "!head -4 /data/yinterian/wikitext-2/wiki.train.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = read_file(PATH/'wiki.train.tokens')\n",
    "valid_lines = read_file(PATH/'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {}\n",
    "words = []\n",
    "for word in vocab:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an \"unk\" already\n",
    "vocab2index['<unk>'] #[y for y in x for x in non_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in train_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in valid_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2051961,), (213886,))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content.shape, val_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1,  0,  2,  3, 49, 54,  1,  9, 52, 19,  0, 18, 39, 52, 46, 70,\n",
       "       15, 27,  5,  1, 34, 65, 40,  9, 35, 15, 50, 64,  7])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Example say we have this dataset and we are using a window size of window=3.\n",
    "\n",
    "Raw Dataset:\n",
    "`the cat is walking in the bedroom`\n",
    "\n",
    "`x                  y \n",
    "the cat is         walking \n",
    "cat is walking     in \n",
    "is walking in      the \n",
    "walking in the     bedroom` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, content, window=5):\n",
    "        self.content = content\n",
    "        self.window = window\n",
    "        self.len = len(self.content) - self.window\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.content[idx : idx + self.window]\n",
    "        y = self.content[idx + self.window]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset([0,1,2,3,4,5,6], window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "([0, 1, 2], 3)\n",
      "([1, 2, 3], 4)\n",
      "([2, 3, 4], 5)\n",
      "([3, 4, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for i in range(4): \n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WikiDataset(train_content, window=5)\n",
    "val_ds = WikiDataset(val_content, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # testing model\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    \"\"\"Lang Model that needs to be regularized.\"\"\"\n",
    "\n",
    "    def __init__(self, V, D, hidden, window=5, dropout=0.5):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.word_emb = nn.Embedding(V, D)\n",
    "        self.linear1 = nn.Linear(window*D, hidden)\n",
    "        self.dense_bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, V)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.dense_bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15928,     6, 14699,    15,   153],\n",
       "         [ 3616,    53,   296,  5184,    34],\n",
       "         [  713,   101,  1488, 10004,  9994],\n",
       "         [  320,    17, 14590,    17,  1572]]),\n",
       " tensor([  17, 2502, 1459,  188]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 3 # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = nn.Embedding(V, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1306,  0.9653, -0.0159],\n",
       "         [-0.0256, -0.8275, -0.4398],\n",
       "         [-1.5326, -1.0188,  2.5852],\n",
       "         [-1.4296,  0.6531, -0.1261],\n",
       "         [-0.5627,  1.3889, -0.3952]],\n",
       "\n",
       "        [[ 0.2710, -0.6399, -0.9767],\n",
       "         [ 0.1841,  0.5853, -0.4152],\n",
       "         [ 1.6365,  0.5396,  0.0575],\n",
       "         [-0.8606,  0.3476,  0.9160],\n",
       "         [ 1.2647, -1.0991, -1.4873]],\n",
       "\n",
       "        [[-0.0752,  0.2613,  0.7435],\n",
       "         [ 0.6072, -1.4787,  0.3401],\n",
       "         [-0.1738, -1.0006, -1.0554],\n",
       "         [-0.8836, -1.0697,  0.3121],\n",
       "         [-0.9620, -0.2892,  0.1314]],\n",
       "\n",
       "        [[-0.0831, -0.7060,  0.5228],\n",
       "         [-1.0072, -1.3731,  0.2329],\n",
       "         [-0.4790, -1.4167, -0.1525],\n",
       "         [-1.0072, -1.3731,  0.2329],\n",
       "         [ 0.1868,  0.5473,  0.9505]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = word_emb(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1306,  0.9653, -0.0159, -0.0256, -0.8275, -0.4398, -1.5326, -1.0188,\n",
       "          2.5852, -1.4296,  0.6531, -0.1261, -0.5627,  1.3889, -0.3952],\n",
       "        [ 0.2710, -0.6399, -0.9767,  0.1841,  0.5853, -0.4152,  1.6365,  0.5396,\n",
       "          0.0575, -0.8606,  0.3476,  0.9160,  1.2647, -1.0991, -1.4873],\n",
       "        [-0.0752,  0.2613,  0.7435,  0.6072, -1.4787,  0.3401, -0.1738, -1.0006,\n",
       "         -1.0554, -0.8836, -1.0697,  0.3121, -0.9620, -0.2892,  0.1314],\n",
       "        [-0.0831, -0.7060,  0.5228, -1.0072, -1.3731,  0.2329, -0.4790, -1.4167,\n",
       "         -0.1525, -1.0072, -1.3731,  0.2329,  0.1868,  0.5473,  0.9505]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(x1.shape[0], -1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2578, -0.3228, -0.1330,  ...,  0.3378,  0.0733, -0.0093],\n",
       "        [ 0.3104, -0.2697, -0.2017,  ...,  0.4531,  0.2132,  0.2790],\n",
       "        [ 0.3274, -0.2656, -0.1990,  ...,  0.2809,  0.2792, -0.1894],\n",
       "        [ 0.3502, -0.1914, -0.2320,  ...,  0.1110, -0.0466,  0.9675]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`F.cross_entropy` combines `log_softmax` and `nll_loss` in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        total_loss = 0.\n",
    "        for x, y in train_dl:\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss +=  x.size(0)* loss.item()\n",
    "            total += x.size(0)\n",
    "        val_loss = val_metrics(model)\n",
    "        print(\"train_loss %.3f val_loss %.3f perplexity %.3f\" % (total_loss/total, \n",
    "                                                                 val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    total_loss = 0.\n",
    "    for x,y in val_dl:\n",
    "        x = torch.LongTensor(x).cuda()\n",
    "        y = torch.LongTensor(y).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        total_loss +=  x.size(0)* loss.item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.430189487070546"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 6.581 val_loss 5.870 perplexity 354.240\n",
      "train_loss 5.824 val_loss 5.725 perplexity 306.337\n",
      "train_loss 5.618 val_loss 5.675 perplexity 291.513\n",
      "train_loss 5.491 val_loss 5.654 perplexity 285.335\n",
      "train_loss 5.399 val_loss 5.646 perplexity 283.159\n",
      "train_loss 5.327 val_loss 5.647 perplexity 283.465\n",
      "train_loss 5.268 val_loss 5.658 perplexity 286.612\n",
      "train_loss 5.219 val_loss 5.671 perplexity 290.464\n",
      "train_loss 5.177 val_loss 5.682 perplexity 293.483\n",
      "train_loss 5.140 val_loss 5.693 perplexity 296.928\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, PATH/\"model10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 5.027 val_loss 5.715 perplexity 303.370\n",
      "train_loss 4.999 val_loss 5.733 perplexity 309.009\n",
      "train_loss 4.988 val_loss 5.746 perplexity 312.961\n",
      "train_loss 4.980 val_loss 5.760 perplexity 317.437\n",
      "train_loss 4.973 val_loss 5.769 perplexity 320.059\n",
      "train_loss 4.967 val_loss 5.780 perplexity 323.618\n",
      "train_loss 4.962 val_loss 5.792 perplexity 327.559\n",
      "train_loss 4.958 val_loss 5.799 perplexity 329.857\n",
      "train_loss 4.953 val_loss 5.807 perplexity 332.630\n",
      "train_loss 4.949 val_loss 5.815 perplexity 335.326\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model20.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
