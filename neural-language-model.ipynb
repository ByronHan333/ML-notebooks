{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with a language model\n",
    "This model is a version of the one introduced in 2003 by Bengio et all [here](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded here.\n",
    "`https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/wiki.test.tokens'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model20.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model10.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/model.pth'),\n",
       " PosixPath('/data/yinterian/wikitext-2/mode117.pth')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/data/yinterian/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n"
     ]
    }
   ],
   "source": [
    "!head -4 /data/yinterian/wikitext-2/wiki.train.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = read_file(PATH/'wiki.train.tokens')\n",
    "valid_lines = read_file(PATH/'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {}\n",
    "words = []\n",
    "for word in vocab:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an \"unk\" already\n",
    "vocab2index['<unk>'] #[y for y in x for x in non_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in train_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_content = np.array([vocab2index.get(w, vocab2index[\"<unk>\"]) for line in valid_lines for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2051961,), (213886,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content.shape, val_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  0,  1,  3, 46, 66,  2, 53, 27, 36,  0, 76,  5, 27, 79, 65,\n",
       "       47, 72, 37,  2, 18, 28, 54, 53, 38, 47, 39, 61, 52])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Example say we have this dataset and we are using a window size of window=3.\n",
    "\n",
    "Raw Dataset:\n",
    "`the cat is walking in the bedroom`\n",
    "\n",
    "`x                  y \n",
    "the cat is         walking \n",
    "cat is walking     in \n",
    "is walking in      the \n",
    "walking in the     bedroom` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, content, window=5):\n",
    "        self.content = content\n",
    "        self.window = window\n",
    "        self.len = len(self.content) - self.window\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.content[idx : idx + self.window]\n",
    "        y = self.content[idx + self.window]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiDataset([0,1,2,3,4,5,6], window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "([0, 1, 2], 3)\n",
      "([1, 2, 3], 4)\n",
      "([2, 3, 4], 5)\n",
      "([3, 4, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for i in range(4): \n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WikiDataset(train_content, window=5)\n",
    "val_ds = WikiDataset(val_content, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # testing model\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    \"\"\"Lang Model that needs to be regularized.\"\"\"\n",
    "\n",
    "    def __init__(self, V, D, hidden, window=5, dropout=0.5):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.word_emb = nn.Embedding(V, D)\n",
    "        self.linear1 = nn.Linear(window*D, hidden)\n",
    "        self.dense_bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, V)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.dense_bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   92,  1785, 15820,    47,   234],\n",
       "         [   37,   108, 29375,   449,    18],\n",
       "         [ 5177,    82, 15644,    47,  6814],\n",
       "         [   36,    47,    82,   843,  9748]]),\n",
       " tensor([ 479, 4287,   47,  690]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 3 # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = nn.Embedding(V, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2512, -0.5158, -0.2631],\n",
       "         [ 0.5185, -1.3206, -1.0044],\n",
       "         [ 0.8041, -0.9139,  0.3286],\n",
       "         [-0.5446, -0.2846,  0.0880],\n",
       "         [ 0.5323,  0.9674, -0.3900]],\n",
       "\n",
       "        [[-1.1816, -0.0356, -0.0244],\n",
       "         [-1.8873, -0.5559, -2.4781],\n",
       "         [-1.2179,  0.0639,  0.5769],\n",
       "         [ 1.7267,  2.3174, -1.7062],\n",
       "         [-2.8953, -0.4923,  0.2094]],\n",
       "\n",
       "        [[-0.5084, -0.0363, -0.0090],\n",
       "         [-0.4232, -2.1233, -0.4622],\n",
       "         [ 0.3782,  0.7960,  0.0047],\n",
       "         [-0.5446, -0.2846,  0.0880],\n",
       "         [ 2.5962,  0.1487,  0.3901]],\n",
       "\n",
       "        [[ 0.2585,  0.4235, -0.5735],\n",
       "         [-0.5446, -0.2846,  0.0880],\n",
       "         [-0.4232, -2.1233, -0.4622],\n",
       "         [-0.1327,  0.4522,  0.6747],\n",
       "         [ 0.0530,  1.0400, -0.0339]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = word_emb(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2512, -0.5158, -0.2631,  0.5185, -1.3206, -1.0044,  0.8041, -0.9139,\n",
       "          0.3286, -0.5446, -0.2846,  0.0880,  0.5323,  0.9674, -0.3900],\n",
       "        [-1.1816, -0.0356, -0.0244, -1.8873, -0.5559, -2.4781, -1.2179,  0.0639,\n",
       "          0.5769,  1.7267,  2.3174, -1.7062, -2.8953, -0.4923,  0.2094],\n",
       "        [-0.5084, -0.0363, -0.0090, -0.4232, -2.1233, -0.4622,  0.3782,  0.7960,\n",
       "          0.0047, -0.5446, -0.2846,  0.0880,  2.5962,  0.1487,  0.3901],\n",
       "        [ 0.2585,  0.4235, -0.5735, -0.5446, -0.2846,  0.0880, -0.4232, -2.1233,\n",
       "         -0.4622, -0.1327,  0.4522,  0.6747,  0.0530,  1.0400, -0.0339]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(x1.shape[0], -1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0943,  0.0098, -0.1051,  ..., -0.1925,  0.2966,  0.0325],\n",
       "        [-0.3251, -0.0107, -0.4621,  ..., -0.1111,  0.4544, -0.1570],\n",
       "        [-0.1425,  0.2231,  0.1544,  ..., -0.2237,  0.2928, -0.1015],\n",
       "        [-0.2112,  0.0486, -0.1693,  ..., -0.0402, -0.0255,  0.1471]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`F.cross_entropy` combines `log_softmax` and `nll_loss` in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        total_loss = 0.\n",
    "        for x, y in train_dl:\n",
    "            x = torch.LongTensor(x).cuda()\n",
    "            y = torch.LongTensor(y).cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss +=  x.size(0)* loss.item()\n",
    "            total += x.size(0)\n",
    "        val_loss = val_metrics(model)\n",
    "        print(\"train_loss %.3f val_loss %.3f perplexity %.3f\" % (total_loss/total, \n",
    "                                                                 val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    total_loss = 0.\n",
    "    for x,y in val_dl:\n",
    "        x = torch.LongTensor(x).cuda()\n",
    "        y = torch.LongTensor(y).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        total_loss +=  x.size(0)* loss.item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 50\n",
    "model = LangModel(V, D, hidden=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.421753756367842"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 6.586 val_loss 5.883 perplexity 358.788\n",
      "train_loss 5.830 val_loss 5.724 perplexity 306.236\n",
      "train_loss 5.622 val_loss 5.675 perplexity 291.376\n",
      "train_loss 5.495 val_loss 5.653 perplexity 285.217\n",
      "train_loss 5.404 val_loss 5.648 perplexity 283.629\n",
      "train_loss 5.333 val_loss 5.655 perplexity 285.705\n",
      "train_loss 5.275 val_loss 5.663 perplexity 287.979\n",
      "train_loss 5.226 val_loss 5.672 perplexity 290.484\n",
      "train_loss 5.184 val_loss 5.689 perplexity 295.519\n",
      "train_loss 5.146 val_loss 5.700 perplexity 298.914\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, PATH/\"model10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 5.035 val_loss 5.721 perplexity 305.320\n",
      "train_loss 5.008 val_loss 5.744 perplexity 312.243\n",
      "train_loss 4.997 val_loss 5.757 perplexity 316.547\n",
      "train_loss 4.989 val_loss 5.771 perplexity 320.755\n",
      "train_loss 4.982 val_loss 5.781 perplexity 324.137\n",
      "train_loss 4.976 val_loss 5.788 perplexity 326.409\n",
      "train_loss 4.971 val_loss 5.805 perplexity 331.821\n",
      "train_loss 4.966 val_loss 5.812 perplexity 334.325\n",
      "train_loss 4.962 val_loss 5.817 perplexity 336.089\n",
      "train_loss 4.958 val_loss 5.825 perplexity 338.750\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, PATH/\"model20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
