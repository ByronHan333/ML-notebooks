{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will write a matrix factorization model in pytorch to solve a recommendation problem. Then we will write a more general neural model for the same problem.\n",
    "\n",
    "Collaborative filtering: systems recommend items based on similarity measures between users and/or items. The items recommended to a user are those preferred by similar users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MovieLens dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. https://grouplens.org/datasets/movielens/. To get the data:\n",
    "\n",
    "`wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/links.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/tags.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/ratings.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/README.txt'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/movies.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/Users/yinterian/teaching/ML-2/data/ml-latest-small/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,31,2.5,1260759144\r",
      "\r\n",
      "1,1029,3.0,1260759179\r",
      "\r\n",
      "1,1061,3.0,1260759182\r",
      "\r\n",
      "1,1129,2.0,1260759185\r",
      "\r\n",
      "1,1172,4.0,1260759205\r",
      "\r\n",
      "1,1263,2.0,1260759151\r",
      "\r\n",
      "1,1287,2.0,1260759187\r",
      "\r\n",
      "1,1293,2.0,1260759148\r",
      "\r\n",
      "1,1339,3.5,1260759125\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head /Users/yinterian/teaching/deeplearning/data/ml-latest-small/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv into pandas\n",
    "data = pd.read_csv(PATH/\"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data\n",
    "We enconde the data to have contiguous ids for users and movies. You can think about this as a categorical encoding of our two categorical variables userId and movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation before encoding\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a handy function modified from fast.ai\n",
    "def proc_col(col, train_col=None):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"userId\", \"movieId\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(df[col_name], train_col)\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating\n",
       "0        0        0       4\n",
       "1        0        1       5\n",
       "2        1        1       5\n",
       "3        1        2       3\n",
       "4        2        0       4\n",
       "5        2        1       4\n",
       "6        3        0       5\n",
       "7        3        3       2\n",
       "8        4        0       1\n",
       "9        4        3       4\n",
       "10       5        3       5\n",
       "11       6        1       1\n",
       "12       6        3       3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check my implementation\n",
    "df_t = pd.read_csv(\"test_data/tiny_training2.csv\")\n",
    "df_v = pd.read_csv(\"test_data/tiny_val2.csv\")\n",
    "df_t_e = encode_data(df_t)\n",
    "df_v_e = encode_data(df_v, df_t)\n",
    "df_v_e\n",
    "df_t_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = encode_data(train)\n",
    "df_val = encode_data(val, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.8846, -0.6033, -1.7295],\n",
       "        [-0.0655, -0.3334,  0.7959],\n",
       "        [-1.6476,  0.5635,  0.3031],\n",
       "        [-0.6678,  2.0692, -0.1898],\n",
       "        [ 0.8253, -0.2421,  0.5325],\n",
       "        [-0.5016,  0.8493,  1.2077],\n",
       "        [ 0.0506,  0.7678, -0.2365],\n",
       "        [ 0.8921, -0.2085, -0.7218],\n",
       "        [ 0.5329,  0.8161, -2.1785],\n",
       "        [ 1.2591, -0.7965, -1.1599]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 users or items embedding size 3\n",
    "# embedding will be initialized at random\n",
    "embed = nn.Embedding(10, 3)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0655, -0.3334,  0.7959],\n",
       "         [-2.8846, -0.6033, -1.7295],\n",
       "         [-0.0655, -0.3334,  0.7959],\n",
       "         [ 0.8253, -0.2421,  0.5325],\n",
       "         [-0.5016,  0.8493,  1.2077],\n",
       "         [-0.0655, -0.3334,  0.7959]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "# can you see that some vectors are the same?\n",
    "a = torch.LongTensor([[1,0,1,4,5,1]])\n",
    "embed(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initlializing weights\n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating\n",
       "0        0        0       4\n",
       "1        0        1       5\n",
       "2        1        1       5\n",
       "3        1        2       3\n",
       "4        2        0       4\n",
       "5        2        1       4\n",
       "6        3        0       5\n",
       "7        3        3       2\n",
       "8        4        0       1\n",
       "9        4        3       4\n",
       "10       5        3       5\n",
       "11       6        1       1\n",
       "12       6        3       3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 7\n",
    "num_items = 4\n",
    "emb_size = 3\n",
    "\n",
    "user_emb = nn.Embedding(num_users, emb_size)\n",
    "item_emb = nn.Embedding(num_items, emb_size)\n",
    "users = torch.LongTensor(df_t_e.userId.values)\n",
    "items = torch.LongTensor(df_t_e.movieId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = user_emb(users)\n",
    "V = item_emb(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5304, -0.6472,  0.4554],\n",
       "        [ 0.5304, -0.6472,  0.4554],\n",
       "        [ 0.1543, -0.2089, -0.3614],\n",
       "        [ 0.1543, -0.2089, -0.3614],\n",
       "        [ 0.0763, -0.8429,  0.0911],\n",
       "        [ 0.0763, -0.8429,  0.0911],\n",
       "        [-1.1417, -1.1717, -0.4392],\n",
       "        [-1.1417, -1.1717, -0.4392],\n",
       "        [ 1.9817, -1.9040,  0.4578],\n",
       "        [ 1.9817, -1.9040,  0.4578],\n",
       "        [ 1.3078,  0.2057, -0.2978],\n",
       "        [-0.3635, -0.2366, -1.4741],\n",
       "        [-0.3635, -0.2366, -1.4741]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0763, -0.3313,  0.3695],\n",
       "        [-0.0621,  0.4944,  0.6128],\n",
       "        [-0.0181,  0.1596, -0.4864],\n",
       "        [-0.1408, -0.1944, -0.1817],\n",
       "        [-0.0110, -0.4314,  0.0739],\n",
       "        [-0.0089,  0.6438,  0.1226],\n",
       "        [ 0.1643, -0.5997, -0.3564],\n",
       "        [-0.2089, -0.2127,  0.3451],\n",
       "        [-0.2851, -0.9745,  0.3715],\n",
       "        [ 0.3627, -0.3456, -0.3597],\n",
       "        [ 0.2393,  0.0373,  0.2340],\n",
       "        [ 0.0425,  0.1808, -1.9838],\n",
       "        [-0.0665, -0.0430,  1.1580]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element wise multiplication\n",
    "U*V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0381,  1.0451, -0.3449, -0.5169, -0.3685,  0.7575, -0.7919,\n",
       "        -0.0766, -0.8881, -0.3426,  0.5107, -1.7605,  1.0486])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we want is a dot product per row\n",
    "(U*V).sum(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 8442\n"
     ]
    }
   ],
   "source": [
    "num_users = len(df_train.userId.unique())\n",
    "num_items = len(df_train.movieId.unique())\n",
    "print(num_users, num_items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are not using data loaders because our data fits well in memory\n",
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(df_train.userId.values)  #.cuda()\n",
    "        items = torch.LongTensor(df_train.movieId.values) #.cuda()\n",
    "        ratings = torch.FloatTensor(df_train.rating.values)  #.cuda()\n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        testloss = test_loss(model, unsqueeze)\n",
    "        print(\"train loss %.3f test loss %.3f\" % (loss.item(), testloss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79799])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([79799, 1])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what unsqueeze does\n",
    "ratings = torch.FloatTensor(df_train.rating.values)\n",
    "print(ratings.shape)\n",
    "ratings = ratings.unsqueeze(1) #.cuda()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(model, unsqueeze=False):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(df_val.userId.values) # .cuda()\n",
    "    items = torch.LongTensor(df_val.movieId.values) #.cuda()\n",
    "    ratings = torch.FloatTensor(df_val.rating.values) #.cuda()\n",
    "    if unsqueeze:\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "    y_hat = model(users, items)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF(num_users, num_items, emb_size=100)  # if you have a GPU .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.22977066040039\n",
      "5.127340793609619\n",
      "2.357693672180176\n",
      "3.313488721847534\n",
      "0.899741530418396\n",
      "1.8573846817016602\n",
      "2.7810661792755127\n",
      "2.2830049991607666\n",
      "1.1453200578689575\n",
      "0.9064176082611084\n",
      "1.6611082553863525\n",
      "1.4776304960250854\n",
      "0.8247122168540955\n",
      "0.909933865070343\n",
      "1.3347585201263428\n",
      "1.392578125\n",
      "1.0245131254196167\n",
      "0.6999652981758118\n",
      "0.8274641633033752\n",
      "1.0505080223083496\n",
      "test loss 1.131 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765497803688049\n",
      "0.6484661102294922\n",
      "0.6463428735733032\n",
      "0.6898732781410217\n",
      "0.6919195055961609\n",
      "0.6609521508216858\n",
      "0.6276931166648865\n",
      "0.6107308864593506\n",
      "0.6090696454048157\n",
      "0.6101611256599426\n",
      "0.6040680408477783\n",
      "0.5906494855880737\n",
      "0.576405942440033\n",
      "0.5673468708992004\n",
      "0.5642893314361572\n",
      "test loss 0.847 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563148021697998\n",
      "0.5561341047286987\n",
      "0.5504226684570312\n",
      "0.5457829236984253\n",
      "0.5419900417327881\n",
      "0.5387914180755615\n",
      "0.536023736000061\n",
      "0.5335185527801514\n",
      "0.531192421913147\n",
      "0.5289515256881714\n",
      "0.5267682671546936\n",
      "0.5246036648750305\n",
      "0.5224275588989258\n",
      "0.5202339291572571\n",
      "0.5180172920227051\n",
      "test loss 0.816 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5157772302627563\n",
      "0.5129480361938477\n",
      "0.5102515816688538\n",
      "0.5076241493225098\n",
      "0.5050358176231384\n",
      "0.5024455785751343\n",
      "0.49983420968055725\n",
      "0.49720141291618347\n",
      "0.494540274143219\n",
      "0.49184277653694153\n",
      "0.4891185164451599\n",
      "0.4863676428794861\n",
      "0.48359766602516174\n",
      "0.48080265522003174\n",
      "0.47798213362693787\n",
      "test loss 0.818 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF_bias, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        return (U*V).sum(1) +  b_u  + b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF_bias(num_users, num_items, emb_size=100) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.235 test loss 9.547\n",
      "train loss 9.456 test loss 4.670\n",
      "train loss 4.613 test loss 1.256\n",
      "train loss 1.227 test loss 2.546\n",
      "train loss 2.460 test loss 4.070\n",
      "train loss 3.887 test loss 2.838\n",
      "train loss 2.613 test loss 1.393\n",
      "train loss 1.157 test loss 1.057\n",
      "train loss 0.822 test loss 1.538\n",
      "train loss 1.312 test loss 2.128\n",
      "train loss 1.915 test loss 2.413\n",
      "train loss 2.211 test loss 2.300\n",
      "train loss 2.108 test loss 1.880\n",
      "train loss 1.694 test loss 1.347\n",
      "train loss 1.164 test loss 0.945\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.766 test loss 0.860\n",
      "train loss 0.673 test loss 0.865\n",
      "train loss 0.683 test loss 0.844\n",
      "train loss 0.671 test loss 0.822\n",
      "train loss 0.657 test loss 0.823\n",
      "train loss 0.660 test loss 0.833\n",
      "train loss 0.668 test loss 0.834\n",
      "train loss 0.665 test loss 0.827\n",
      "train loss 0.653 test loss 0.822\n",
      "train loss 0.641 test loss 0.825\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.636 test loss 0.822\n",
      "train loss 0.633 test loss 0.821\n",
      "train loss 0.631 test loss 0.819\n",
      "train loss 0.630 test loss 0.818\n",
      "train loss 0.628 test loss 0.818\n",
      "train loss 0.627 test loss 0.817\n",
      "train loss 0.626 test loss 0.817\n",
      "train loss 0.624 test loss 0.816\n",
      "train loss 0.623 test loss 0.816\n",
      "train loss 0.622 test loss 0.815\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.621 test loss 0.815\n",
      "train loss 0.619 test loss 0.814\n",
      "train loss 0.617 test loss 0.814\n",
      "train loss 0.616 test loss 0.814\n",
      "train loss 0.614 test loss 0.813\n",
      "train loss 0.613 test loss 0.813\n",
      "train loss 0.611 test loss 0.813\n",
      "train loss 0.609 test loss 0.813\n",
      "train loss 0.608 test loss 0.812\n",
      "train loss 0.606 test loss 0.812\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.604 test loss 0.812\n",
      "train loss 0.602 test loss 0.812\n",
      "train loss 0.600 test loss 0.812\n",
      "train loss 0.598 test loss 0.812\n",
      "train loss 0.596 test loss 0.812\n",
      "train loss 0.594 test loss 0.812\n",
      "train loss 0.592 test loss 0.812\n",
      "train loss 0.590 test loss 0.811\n",
      "train loss 0.588 test loss 0.811\n",
      "train loss 0.586 test loss 0.811\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these models are susceptible to weight initialization, optimization algorithm and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here there is no matrix multiplication, we could potentially make the embeddings \n",
    "# for users and items of different sizes.\n",
    "# Here we could get better results by keep playing with regularization.\n",
    "    \n",
    "class CollabFNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100, n_hidden=20):\n",
    "        super(CollabFNet, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.lin1 = nn.Linear(emb_size*2, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.drop2 = nn.Dropout(0.0)\n",
    "        self.dense_bn = nn.BatchNorm1d(n_hidden)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        x = torch.cat([U, V], dim=1)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.dense_bn(self.lin1(x)))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollabFNet(num_users, num_items, emb_size=120, n_hidden=40) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.927 test loss 5.797\n",
      "train loss 7.231 test loss 4.354\n",
      "train loss 2.362 test loss 20.705\n",
      "train loss 1.667 test loss 33.821\n",
      "train loss 3.590 test loss 25.512\n",
      "train loss 2.669 test loss 13.091\n",
      "train loss 1.299 test loss 5.303\n",
      "train loss 0.888 test loss 2.151\n",
      "train loss 1.183 test loss 1.264\n",
      "train loss 1.540 test loss 1.105\n",
      "train loss 1.641 test loss 1.121\n",
      "train loss 1.467 test loss 1.229\n",
      "train loss 1.152 test loss 1.476\n",
      "train loss 0.870 test loss 1.885\n",
      "train loss 0.750 test loss 2.372\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.1, wd=1e-5, unsqueeze=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.826 test loss 1.577\n",
      "train loss 0.712 test loss 1.161\n",
      "train loss 0.677 test loss 0.972\n",
      "train loss 0.689 test loss 0.903\n",
      "train loss 0.701 test loss 0.882\n",
      "train loss 0.700 test loss 0.875\n",
      "train loss 0.687 test loss 0.871\n",
      "train loss 0.667 test loss 0.867\n",
      "train loss 0.654 test loss 0.865\n",
      "train loss 0.645 test loss 0.863\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.641 test loss 0.865\n",
      "train loss 0.648 test loss 0.853\n",
      "train loss 0.631 test loss 0.845\n",
      "train loss 0.626 test loss 0.844\n",
      "train loss 0.627 test loss 0.844\n",
      "train loss 0.621 test loss 0.851\n",
      "train loss 0.612 test loss 0.866\n",
      "train loss 0.607 test loss 0.881\n",
      "train loss 0.605 test loss 0.887\n",
      "train loss 0.601 test loss 0.882\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.598 test loss 0.873\n",
      "train loss 0.593 test loss 0.866\n",
      "train loss 0.594 test loss 0.860\n",
      "train loss 0.595 test loss 0.856\n",
      "train loss 0.596 test loss 0.854\n",
      "train loss 0.593 test loss 0.853\n",
      "train loss 0.592 test loss 0.853\n",
      "train loss 0.591 test loss 0.853\n",
      "train loss 0.595 test loss 0.853\n",
      "train loss 0.590 test loss 0.854\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.591 test loss 0.854\n",
      "train loss 0.589 test loss 0.854\n",
      "train loss 0.590 test loss 0.854\n",
      "train loss 0.586 test loss 0.853\n",
      "train loss 0.589 test loss 0.853\n",
      "train loss 0.587 test loss 0.852\n",
      "train loss 0.585 test loss 0.852\n",
      "train loss 0.586 test loss 0.852\n",
      "train loss 0.584 test loss 0.852\n",
      "train loss 0.586 test loss 0.852\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.584 test loss 0.851\n",
      "train loss 0.586 test loss 0.853\n",
      "train loss 0.584 test loss 0.852\n",
      "train loss 0.583 test loss 0.851\n",
      "train loss 0.581 test loss 0.851\n",
      "train loss 0.583 test loss 0.851\n",
      "train loss 0.582 test loss 0.851\n",
      "train loss 0.582 test loss 0.852\n",
      "train loss 0.582 test loss 0.852\n",
      "train loss 0.581 test loss 0.851\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.719 test loss 0.829\n",
      "train loss 0.714 test loss 0.828\n",
      "train loss 0.711 test loss 0.827\n",
      "train loss 0.710 test loss 0.827\n",
      "train loss 0.711 test loss 0.827\n",
      "train loss 0.710 test loss 0.827\n",
      "train loss 0.717 test loss 0.827\n",
      "train loss 0.710 test loss 0.827\n",
      "train loss 0.712 test loss 0.827\n",
      "train loss 0.711 test loss 0.826\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* use t-sne to visualize embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "* Can you use `tags.csv` and `timestamp` to improve your predictions?\n",
    "* Play with the hyperparameters\n",
    "* Look at fastai version of this network and try his transformation https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb\n",
    "* You may need a dataloader if you data is larger. Can you construct a dataset? Here is an example:\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel.html\n",
    "* Work with the largest dataset http://files.grouplens.org/datasets/movielens/ml-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* This notebook is based on [lesson 5 of Jeremy Howard's Deep Learning Course](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
