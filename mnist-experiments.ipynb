{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "Here we load the dataset and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('../data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('../data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method (from fast.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0][:4][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the number of neurons in the hidden unit\n",
    "def get_model(M = 300):\n",
    "    net = nn.Sequential(nn.Linear(28*28, M),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M, 10))\n",
    "    return net #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            batch = images.shape[0] # size of the batch\n",
    "            # Convert torch tensor to Variable, change shape of the input\n",
    "            images = images.view(-1, 28*28) #.cuda()\n",
    "        \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total += batch\n",
    "            sum_loss += batch * loss.item()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, sum_loss/total))\n",
    "                \n",
    "        train_loss = sum_loss/total\n",
    "        print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, train_loss))\n",
    "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
    "        print('Epoch [%d/%d], Valid Accuracy: %.4f, Valid Loss: %.4f' %(epoch+1, num_epochs, val_acc, val_loss))\n",
    "    return val_acc, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy_loss(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)  #.cuda()\n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels.data).sum().item()\n",
    "    return 100 * correct / total, sum_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.11, 2.3332808712005617)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_model()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "model_accuracy_loss(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 0.8138\n",
      "Epoch [1/2], Loss: 0.5971\n",
      "Epoch [1/2], Loss: 0.5093\n",
      "Epoch [1/2], Loss: 0.4700\n",
      "Epoch [1/2], Loss: 0.4516\n",
      "Epoch [1/2], Loss: 0.4277\n",
      "Epoch [1/2], Loss: 0.4069\n",
      "Epoch [1/2], Loss: 0.3899\n",
      "Epoch [1/2], Loss: 0.3821\n",
      "Epoch [1/2], Loss: 0.3724\n",
      "Epoch [1/2], Loss: 0.3679\n",
      "Epoch [1/2], Loss: 0.3625\n",
      "Epoch [1/2], Loss: 0.3573\n",
      "Epoch [1/2], Loss: 0.3533\n",
      "Epoch [1/2], Loss: 0.3473\n",
      "Epoch [1/2], Loss: 0.3446\n",
      "Epoch [1/2], Loss: 0.3421\n",
      "Epoch [1/2], Loss: 0.3383\n",
      "Epoch [1/2], Loss: 0.3335\n",
      "Epoch [1/2], Valid Accuracy: 93.8500, Valid Loss: 0.2417\n",
      "Epoch [2/2], Loss: 0.3286\n",
      "Epoch [2/2], Loss: 0.3265\n",
      "Epoch [2/2], Loss: 0.3233\n",
      "Epoch [2/2], Loss: 0.3219\n",
      "Epoch [2/2], Loss: 0.3206\n",
      "Epoch [2/2], Loss: 0.3178\n",
      "Epoch [2/2], Loss: 0.3157\n",
      "Epoch [2/2], Loss: 0.3143\n",
      "Epoch [2/2], Loss: 0.3127\n",
      "Epoch [2/2], Loss: 0.3116\n",
      "Epoch [2/2], Loss: 0.3086\n",
      "Epoch [2/2], Loss: 0.3067\n",
      "Epoch [2/2], Loss: 0.3049\n",
      "Epoch [2/2], Loss: 0.3036\n",
      "Epoch [2/2], Loss: 0.3025\n",
      "Epoch [2/2], Loss: 0.3023\n",
      "Epoch [2/2], Loss: 0.3010\n",
      "Epoch [2/2], Loss: 0.3002\n",
      "Epoch [2/2], Loss: 0.2998\n",
      "Epoch [2/2], Valid Accuracy: 93.3300, Valid Loss: 0.2718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93.33, 0.27183027888601646, 0.299811094109714)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader, test_loader, num_epochs=2, model=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with L2 regularization\n",
    "To add L2 regularization use the `weight_decay` argument on the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v2(M = 300, p=0):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(28*28, 10))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(M, 10))\n",
    "    return nn.Sequential(*modules) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = get_model_v2(M = 300, p=0.1)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 0.1764\n",
      "Epoch [1/4], Loss: 0.1793\n",
      "Epoch [1/4], Loss: 0.1763\n",
      "Epoch [1/4], Loss: 0.1796\n",
      "Epoch [1/4], Loss: 0.1755\n",
      "Epoch [1/4], Loss: 0.1760\n",
      "Epoch [1/4], Loss: 0.1745\n",
      "Epoch [1/4], Loss: 0.1738\n",
      "Epoch [1/4], Loss: 0.1751\n",
      "Epoch [1/4], Loss: 0.1763\n",
      "Epoch [1/4], Valid Accuracy: 94.9800, Valid Loss: 0.2079\n",
      "Epoch [2/4], Loss: 0.1748\n",
      "Epoch [2/4], Loss: 0.1736\n",
      "Epoch [2/4], Loss: 0.1731\n",
      "Epoch [2/4], Loss: 0.1730\n",
      "Epoch [2/4], Loss: 0.1740\n",
      "Epoch [2/4], Loss: 0.1724\n",
      "Epoch [2/4], Loss: 0.1719\n",
      "Epoch [2/4], Loss: 0.1718\n",
      "Epoch [2/4], Loss: 0.1718\n",
      "Epoch [2/4], Loss: 0.1719\n",
      "Epoch [2/4], Valid Accuracy: 94.7000, Valid Loss: 0.2242\n",
      "Epoch [3/4], Loss: 0.1717\n",
      "Epoch [3/4], Loss: 0.1719\n",
      "Epoch [3/4], Loss: 0.1712\n",
      "Epoch [3/4], Loss: 0.1706\n",
      "Epoch [3/4], Loss: 0.1698\n",
      "Epoch [3/4], Loss: 0.1702\n",
      "Epoch [3/4], Loss: 0.1699\n",
      "Epoch [3/4], Loss: 0.1697\n",
      "Epoch [3/4], Loss: 0.1693\n",
      "Epoch [3/4], Loss: 0.1689\n",
      "Epoch [3/4], Valid Accuracy: 95.0600, Valid Loss: 0.2296\n",
      "Epoch [4/4], Loss: 0.1696\n",
      "Epoch [4/4], Loss: 0.1687\n",
      "Epoch [4/4], Loss: 0.1683\n",
      "Epoch [4/4], Loss: 0.1678\n",
      "Epoch [4/4], Loss: 0.1671\n",
      "Epoch [4/4], Loss: 0.1672\n",
      "Epoch [4/4], Loss: 0.1671\n",
      "Epoch [4/4], Loss: 0.1674\n",
      "Epoch [4/4], Loss: 0.1665\n",
      "Epoch [4/4], Loss: 0.1664\n",
      "Epoch [4/4], Valid Accuracy: 94.6400, Valid Loss: 0.2339\n"
     ]
    }
   ],
   "source": [
    "train_model(train_loader, test_loader, num_epochs=4, model=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
